---
title: "pn: 中文文本資料處理 (W10)"
date: '2019-11-14'
output:
    html_document:
        number_sections: no
        theme: united
        highlight: tango
        toc: yes
        toc_float:
            collapsed: no
        css: style.css
        md_extensions: +implicit_figures
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,
                      results = 'hide',
                      comment = '#>',
                      message = F,
                      out.width = '80%',
                      fig.align = 'center')
library(ggplot2)
```

```{js echo=FALSE}
window.onload = () => {
        document.querySelectorAll('.hide').forEach(ele => {
            ele.className = 'note';
        })
}
```

## 程式小習慣

<div class="hide" data-tag="slide">
1. Changing stuff and seeing what happens
1. (到目前的) 作業都用 Rmd --> 忽略掉使用 Rscript 的好處
1. 建議寫作業的時候可以開一個 Rscript --> 專心把一題作業寫出來 --> 寫完之後再把程式碼複製到 Rmd
    - Rscript 好處：在裡面「做實驗」  
    (e.g., 作業：請勿更動程式碼 --> 盡情的修改程式碼)
1. 另一件事情是關於「寫程式的流程」
    1. (寫作業或考試) Rmd 輸出和 RStudio 裡面看到的結果是不一樣的
    1. (Why) 很多時候，這是因為「先前已經執行過許多程式碼」，  
    所以 R 會記錄下這些程式碼執行的結果，  
    這些預先存在的結果，會**影響**你之後在 console 執行程式碼的結果  
    (最常見的例子是，你在 Console 裡面載入某個 package)
    1. (How to 避免) 為了避免這種情況，  
    在寫程式的過程中，要養成「時常將 Environment 清乾淨的習慣」  
    這樣 (隨時在乾淨的環境下工作) 才比較容易在寫程式的時候找出 bug 在哪  
    (例如，在 Rscript 裡寫完一題作業的時候，就可以先把 Environment 清掉，這樣在寫下一題的時候，這題的執行結果才不會影響到下一題)
    1. 快捷鍵
    1. 流程 (循環)

1. (下載原始碼) 今天實習課的時候，大家就可以試看看這個流程
    - 實習課原始碼沒有 Rmd 檔，但有一份 Rscript
</div>


## Text Mining Frameworks

<div class="hide" data-tag="Slide">
1. text mining 在西方世界發展起來 --> 英文文本, 書寫系統 --> 斷詞
1. flow
    - 文本資料、Metadata、斷詞
    - Document data frame: 好處 --> 轉換成 R 其它 text mining 套件使用的格式
1. Document data frame
1. text mining frameworks
    - tidytext: 
        - 把一個 document 想像成 Bag-of-words (詞彙之間的次序不重要)
            - 例如，分析詞頻 --> 計算文件「情緒詞彙」次數，情緒分析
        - tidyverse --> tidy data frame --> 配合 `dplyr`, `ggplot2` 
    - 傳統 text mining 的作法 (早於 tidytext 許多)
        - 許多文本分析的 Model 實作，仰賴傳統 text mining 套件的資料結構
        - 保留文本當中，詞彙之間的順序和關係
            - 文本分析時，需要這些 context 的資訊 (想看一些特定的詞彙，會出現在哪一些**語境**當中)
</div>

## jiebaR, construct document data frame

<div class="hide" data-tag="slide, demo">
1. jieba: python 中文斷詞套件 --> R 實作
2. 使用簡單，兩三行程式碼

**demo**

1. 
    ```r
    library(jiebaR)
    seg <- worker()
    txt <- "失業的熊讚陪柯文哲看銀翼殺手"
    segment(txt, seg)
    ```

1. `user_dict.txt`

    ```txt
    熊讚
    柯文哲
    銀翼殺手
    ```

1. 

    ```r
    library(jiebaR)
    seg <- worker(user = 'user_dict.txt')
    txt <- "失業的熊讚陪柯文哲看銀翼殺手"
    segment(txt, seg)
    ```
</div>

- **2 min**

<div class="hide" data-tag="slide, demo">
**slide**: 整理成 data frame

1. 錯誤示範 (直接把 `docs` 傳入 segment)

    ```r
    docs <- c(
      "蝴蝶和蜜蜂們帶著花朵的蜜糖回來...",
      "朋友買了一件衣料，綠色的底子帶白色方格...",
      "每天，天剛亮時，我母親便把我喊醒..."
      )
    
    seg <- worker()
    segment(docs, seg)
    ```

1. use for loop

    ```r
    for (i in seq_along(docs)) {
        segged <- segment(docs[i], seg)
        print(segged)
        cat("\n")
    }
    ```
    
    ```r
    for (i in seq_along(docs)) {
        segged <- segment(docs[i], seg)
        segged <- paste0(segged, collapse = " ")
        print(segged)
        cat("\n")
    }
    ```
    
    ```r
    docs_segged <- rep("", length(docs))
    for (i in seq_along(docs)) {
        segged <- segment(docs[i], seg)
        docs_segged[i] <- paste0(segged, collapse = " ")
    }
    docs_segged
    ```

1. Construct data frame

    ```r
    tibble::tibble(
        doc_id = seq_along(docs_segged),
        content = docs_segged,
    )
    ```
</div>

- **2 min**

## 語料簡介

<div class="hide" data-tag="slide">

1. 實驗室學長爬下來的
    - 目的：觀察同婚支持方與反對方的「語言使用」
    - 語料來源：下一代幸福聯盟 vs. 台灣伴侶權益推動聯盟
        - 這兩個組織的官方網站，把網站上的文章爬下來 -->   
        我把它斷好詞，整理成 document data frame
1. 變項說明
</div>


## Tidytext framework

<div class="hide" data-tag="slide, demo">
**slide**

1. `unnest_tokens()`: Document data frame 轉換成 tidytext 所要求的資料結構
    - data frame 格式 one-token-per-document-per-row
        - 一個 token (斷出來的一個詞彙 or 語言單位)，
        - doc df 有兩篇文章：第一篇斷出 38 個詞彙, 第二篇斷出 20 個詞彙
1. 轉換成 tidytext 之後 --> 漂亮 df --> dplyr, ggplot2 處理與視覺化
1. 詞頻表


**demo**

1. Convert to tidytext format

    ```r
    library(tidytext)
    library(dplyr)
    library(ggplot2)
    
    # View docs_df
    glimpse(docs_df)
    
    tidy_text_format <- docs_df %>%
        unnest_tokens(output = "word", input = "content",
                      token = "regex", pattern = " ")
    
    tidy_text_format %>% select(-title)
    ```

1. 比較「相同詞彙」分別在這兩個組織裡面出現的次數

    ```r
    word_freq <- tidy_text_format %>% 
      group_by(topic, word) %>%
      summarise(n = n()) %>%
      arrange(desc(word), topic)
    
    # query
    word_freq %>% filter(word == "同志")
    word_freq %>% filter(word %in% c("同志", "同性戀"))
    
    # Regex
    library(stringr)
    word_freq %>% filter(str_detect(word, pattern = "同(志|性戀)"))
    ```

1. 繪圖
    
    ```r
    word_freq %>% 
        filter(word %in% c("同志", "同性戀")) %>%
        ggplot() +
            geom_bar(aes(topic, n, fill = word), 
                     stat = "identity")
    
    # Position adjustment               
    word_freq %>% 
        filter(word %in% c("同志", "同性戀")) %>%
        ggplot() +
            geom_bar(aes(topic, n, fill = word), 
                     stat = "identity", position = "dodge")            
                     
    ```
</div>

**3 min**

## quanteda framework

<div class="hide" data-tag="slide, demo">
- start from bar chart

**slide**

1. `corpus()`, `tokens()`, `kwic()`

**demo**

```r
# 轉換成 quanteda corpus 格式
qcorp <- corpus(docs_df, docid_field = "id", text_field = "content")

# Key word in context
tokens_obj <- tokens(qcorp, "fastestword")
kwic(tokens_obj, valuetype = "regex", pattern = "同(性戀|志)")

# Add View()
kwic(tokens_obj, valuetype = "regex", pattern = "同(性戀|志)") %>% View()

# Add window=10
kwic(tokens_obj, valuetype = "regex", pattern = "同(性戀|志)", window = 10) %>% View()
```
</div>
